{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5116f71e",
   "metadata": {},
   "source": [
    "# Trabalho sobre Perceptron em Redes Neurais Artificiais\n",
    "\n",
    "Objetivo: Criar um material educacional em formato Jupyter Notebook (.ipynb) abordando os fundamentos do Perceptron, com explicações claras, fórmulas, gráficos, animações e códigos implementados manualmente (sem bibliotecas externas para o Perceptron)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89107fc",
   "metadata": {},
   "source": [
    "# 1-Introdução ao modelo de Perceptron\n",
    "\n",
    "## 1.1-Explicação conceitual\n",
    "O Perceptron é um dos algoritmos de aprendizado de máquina supervisionado mais básicos, proposto por Frank Rosenblatt em 1957. Ele é um classificador linear binário. Isso significa que sua função principal é separar duas classes de dados usando uma linha em duas dimensões, ou um hiperplano em dimensões maiores. O objetivo é encontrar essa fronteira de decisão linear ideal. Ele foi inspirado no neurônio biológico e a sua ideia principal era criar um modelo computacional que pudesse aprender a tomar decisões, assim como um cérebro aprende.\n",
    "\n",
    "### Inspiração Biológica\n",
    "Imagine um neurônio no seu cérebro. Ele recebe sinais de outros neurônios através de \"braços\" chamados dendritos. Esses sinais passam por conexões chamadas sinapses, que podem ser mais fortes ou mais fracas. Se a soma dos sinais que chegam ao neurônio for forte o suficiente, ele \"dispara\" e envia um sinal para outros neurônios através de um \"fio\" chamado axônio.\n",
    "\n",
    "O Perceptron tenta imitar esse processo da seguinte forma:\n",
    "\n",
    "*   **Entradas (Inputs):** Assim como os dendritos recebem sinais, o Perceptron recebe múltiplas entradas. Cada entrada representa uma informação ou característica do que estamos tentando classificar (por exemplo, se estamos tentando classificar fotos de maçãs e bananas, as entradas poderiam ser a cor e o formato da fruta).\n",
    "*   **Pesos (Weights):** Cada entrada tem um peso associado a ela (análogo à força das sinapses). Esses pesos indicam a importância de cada entrada para a decisão do Perceptron. Um peso maior significa que essa entrada tem uma influência maior.\n",
    "*   **Soma Ponderada:** O Perceptron multiplica cada entrada pelo seu peso correspondente e soma todos esses resultados. Isso é como o neurônio biológico somando os sinais que recebe.\n",
    "*   **Bias (Limiar):** É adicionado um valor chamado bias (ou limiar) à soma ponderada. O bias determina o quão fácil é para o Perceptron \"disparar\". Pense nele como um valor mínimo que a soma ponderada precisa atingir para que o Perceptron produza uma saída. No neurônio biológico, isso seria o limiar de ativação.\n",
    "*   **Função de Ativação:** O resultado da soma ponderada mais o bias passa por uma função de ativação. A função de ativação original do Perceptron era uma função degrau. Isso significa que se o resultado da soma for maior que um certo valor (determinado pelo bias e pela função), a saída do Perceptron é um valor (por exemplo, 1, representando uma classe). Caso contrário, a saída é outro valor (por exemplo, 0, representando outra classe). É como o neurônio decidindo se \"dispara\" ou não.\n",
    "\n",
    "### O Processo de Aprendizagem\n",
    "A parte crucial do Perceptron é como ele aprende a tomar as decisões corretas. Inicialmente, os pesos e o bias podem ser definidos de forma aleatória, já que isso permite que eles aprendam a detectar diferentes padrões nos dados. O processo de aprendizagem acontece da seguinte maneira:\n",
    "\n",
    "1.  **Apresentação de Dados:** Mostramos ao Perceptron muitos exemplos dos dados que queremos que ele aprenda a classificar. Cada exemplo tem suas entradas (características) e a resposta correta (o \"rótulo\" da classe).\n",
    "2.  **Previsão:** O Perceptron recebe as entradas de um exemplo, calcula a soma ponderada e aplica a função de ativação para fazer uma previsão sobre a classe a que esse exemplo pertence.\n",
    "3.  **Comparação com a Resposta Correta:** Comparamos a previsão do Perceptron com a resposta correta que sabemos para aquele exemplo.\n",
    "4.  **Ajuste dos Pesos e Bias:** Se a previsão estiver errada, o Perceptron ajusta seus pesos e o bias. A ideia é que o ajuste seja feito de forma que, na próxima vez que um exemplo semelhante for apresentado, a previsão tenha mais chances de ser correta. A regra de aprendizado do Perceptron original era simples: se ele classificasse incorretamente, os pesos seriam ajustados na direção que diminuiria o erro. Por exemplo, se ele classificasse um exemplo como \"maçã\" quando era \"banana\", os pesos associados às características de \"banana\" seriam aumentados, e os pesos associados às características de \"maçã\" seriam diminuídos. O bias também seria ajustado para tornar a ativação mais ou menos provável.\n",
    "5.  **Repetição:** Esse processo de apresentar exemplos, fazer previsões, comparar e ajustar os pesos e o bias é repetido muitas vezes (epochs). O objetivo é que, a cada repetição, o Perceptron melhore suas previsões e se torne capaz de classificar corretamente os dados, mesmo aqueles que ele nunca viu antes.\n",
    "\n",
    "Vale ressaltar ainda que o Perceptron só consegue lidar com problemas linearmente separáveis já que sua fronteira de decisão é sempre um hiperplano (uma linha reta em duas dimensões, um plano em três dimensões e assim por diante). Consequentemente, se um conjunto de dados não pode ser dividido em classes por um hiperplano, ou seja, é não linearmente separável, um único perceptron não conseguirá encontrar uma solução para o problema de classificação."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "enviromment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
